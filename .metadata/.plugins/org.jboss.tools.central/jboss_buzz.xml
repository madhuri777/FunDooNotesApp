<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Narayana Commit Markable Resource: a faultless LRCO for JDBC datasources</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qq44hyr6kbA/narayana-commit-markable-resource.html" /><category term="feed_group_name_jbosstransactions" scheme="searchisko:content:tags" /><category term="feed_name_transactions" scheme="searchisko:content:tags" /><author><name>Ondřej Chaloupka</name></author><id>searchisko:content:id:jbossorg_blog-narayana_commit_markable_resource_a_faultless_lrco_for_jdbc_datasources</id><updated>2018-06-28T06:11:15Z</updated><published>2018-06-28T05:17:00Z</published><content type="html">&lt;p&gt; CMR is neat Narayana feature enabling full XA transaction capability for one non-XA JDBC resource. This gives you a way to engage a database resource to XA transaction even the JDBC driver is not fully XA capable (or you just have a design restriction on it) while transaction data consistency is kept. &lt;/p&gt; &lt;h2&gt;Last resource commit optimization (aka. LRCO)&lt;/h2&gt; &lt;p&gt; Maybe you will say "&lt;i&gt;adding one non-XA resource to a transaction is well-known LRCO optimization&lt;/i&gt;". And you are right. But just partially. The last resource commit optimization (abbreviated as LRCO) provides a way to enlist and process one non-XA datasource to the global transaction managed by the transaction manager. But LRCO contains a pitfall. When the crash of the system (or the connection) happens in particular point of the time, &lt;a href="https://developer.jboss.org/wiki/TwoPhaseCommit2PC"&gt;during two-phase commit processing&lt;/a&gt;, it causes data inconsistency. Namely, the LRCO could be committed while the rest of the resources will be rolled-back. &lt;/p&gt;&lt;p&gt; Let's elaborate a bit on the LRCO failure. Let's say we have a JMS resource where we send a message to a message broker and non-XA JDBC datasource where we save information to the database. &lt;p&gt; &lt;p&gt; &lt;i&gt;NOTE: The example refers to the Narayana two-phase commit implemenation.&lt;/i&gt; &lt;br/&gt;&lt;br/&gt; &lt;ol&gt; &lt;li&gt;updating the database with &lt;code&gt;INSERT INTO&lt;/code&gt; SQL command, enlisting LRCO resource under the transaction&lt;/li&gt; &lt;li&gt;sending a message to the JMS broker, enlisting the JMS resource to the transaction&lt;/li&gt; &lt;li&gt;Narayana starts the two phase commit processing&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; is called to JMS XA resource, the transaction log is stored at the JMS broker side&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; phase for the LRCO means to call &lt;code&gt;commit&lt;/code&gt; at the non-XA datasource. That call makes the data changes visible to the outer world.&lt;/li&gt; &lt;li&gt;crash of the Narayana JVM occurs before the Narayana can preserve information of commit to its transaction log store&lt;/li&gt; &lt;li&gt;after the Narayana restarts there is no notion about the existence of any transaction thus the prepared JMS resource is rolled-back during transaction recovery&lt;/li&gt; &lt;/ol&gt; &lt;p&gt; &lt;i&gt;Note:&lt;/i&gt; roll-backing of the JMS resource is caused by &lt;a href="http://narayana.io//docs/product/index.html#two-phase-variants"&gt;presumed abort strategy&lt;/a&gt; applied in the Narayana. If transaction manager does do not apply the presumed abort then you end ideally not better than in the &lt;a href="http://jbossts.blogspot.com/2011/03/heuristics-and-why-you-need-to-know.html"&gt;transaction heuristic state&lt;/a&gt;. &lt;/p&gt;&lt;/p&gt; &lt;p&gt; The LRCO processing is about ordering the LRCO resource as the last during the &lt;a href="https://developer.jboss.org/wiki/TwoPhaseCommit2PC"&gt;transaction manager 2PC&lt;/a&gt; &lt;code&gt;prepare&lt;/code&gt; phase. At place where transaction normally calls &lt;code&gt;prepare&lt;/code&gt; at &lt;code&gt;XAResource&lt;/code&gt;s there is called &lt;code&gt;commit&lt;/code&gt; at the LRCO's underlaying non-XA resource. &lt;br/&gt; Then during the transaction manager &lt;code&gt;commit&lt;/code&gt; phase there is called nothing for the LRCO. &lt;/p&gt; &lt;h2&gt;Commit markable resource (aka. CMR)&lt;/h2&gt;&lt;p&gt; The Commit Markable Resource, abbreviated as &lt;code&gt;CMR&lt;/code&gt;, is an enhancement of the last resource commit optimization applicable on the JDBC resources. The CMR approach achieves capabilities similar to XA by demanding special database table (normally named &lt;code&gt;xids&lt;/code&gt;) that is accessible for transaction manager to write and to read via the configured CMR datasource. &lt;/p&gt; &lt;p&gt; Let's demonstrate the CMR behavior at the example (reusing setup from the previous one). &lt;ol&gt; &lt;li&gt;updating the database with &lt;code&gt;INSERT INTO&lt;/code&gt; SQL command, enlisting the CMR resource under the transaction&lt;/li&gt; &lt;li&gt;sending a message to the JMS broker, enlisting the JMS resource to the transaction&lt;/li&gt; &lt;li&gt;Narayana starts the two phase commit processing&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; on CMR saves information about prepare to the &lt;code&gt;xids&lt;/code&gt; table&lt;/li&gt; &lt;li&gt;&lt;code&gt;prepare&lt;/code&gt; is called to JMS XA resource, the transaction log is stored at the JMS broker side&lt;/li&gt; &lt;li&gt;&lt;code&gt;commit&lt;/code&gt; on CMR means calling commit on underlaying non-XA datasource&lt;/li&gt; &lt;li&gt;&lt;code&gt;commit&lt;/code&gt; on JMS XA resource means commit on the XA JMS resource and thus the message being visible at the queue, the proper transaction log is removed at the JMS broker side&lt;/li&gt; &lt;li&gt;Narayana two phase commit processing ends&lt;/li&gt; &lt;/ol&gt;&lt;/p&gt; &lt;p&gt; From what you can see here the difference from the LRCO example is that the CMR resource is not ordered as last in the resource processing but it's ordered as the first one. The CMR prepare does not mean committing the work as in case of the LRCO but it means saving information about that CMR is considered to be prepared into the database &lt;code&gt;xids&lt;/code&gt; table. &lt;br/&gt; As the CMR is ordered as the first resource for processing it's taken as first during the commit phase too. The commit call then means to call &lt;code&gt;commit&lt;/code&gt; at the underlying database connection. The &lt;code&gt;xids&lt;/code&gt; table is not cleaned at that phase and it's normally responsibility of &lt;code&gt;CommitMarkableResourceRecordRecoveryModule&lt;/code&gt; to process the garbage collection of records in the &lt;code&gt;xids&lt;/code&gt; table (see more below). &lt;/p&gt; &lt;p&gt; The main fact to understand is that CMR resource is considered as &lt;i&gt;fully prepared&lt;/i&gt; only after the &lt;code&gt;commit&lt;/code&gt; is processed (meaning commit on the underlaying non-XA JDBC datasource). Till that time the transaction is considered as &lt;b&gt;not&lt;/b&gt; prepared and will be processed with rollback by the transaction recovery. &lt;/p&gt; &lt;p&gt; &lt;i&gt;NOTE:&lt;/i&gt; the term &lt;i&gt;fully prepared&lt;/i&gt; considers the standard XA two-phase commit processing. If the transaction manager finishes with the &lt;code&gt;prepare&lt;/code&gt; phase, aka. prepare is called on all transaction participants, the transaction is counted as prepared and &lt;code&gt;commit&lt;/code&gt; is expected to be called on each participant. &lt;/p&gt; &lt;p&gt; It's important to note that the correct processing of failures in transactions which contain CMR resources is responsibility of the special &lt;a href="https://jbossts.blogspot.com/2018/01/narayana-periodic-recovery-of-xa.html"&gt;periodic recovery module&lt;/a&gt; &lt;code&gt;CommitMarkableResourceRecordRecoveryModule&lt;/code&gt;. It has to be configured as &lt;b&gt;the first&lt;/b&gt; in the recovery module list as it needs to check and eventually process all the XA resources belonging to the transaction which contains the CMR resource (the recovery modules are processed in the order they were configured). You can check &lt;a href="https://github.com/wildfly/wildfly/blob/13.0.0.Final/transactions/src/main/java/org/jboss/as/txn/service/ArjunaRecoveryManagerService.java#L104"&gt;here how this is set up in WildFly&lt;/a&gt;. &lt;br/&gt; The CMR recovery module knows about the existence of the CMR resource from the record saved in the &lt;code&gt;xids&lt;/code&gt; table. From that it's capable to pair all the resources belonging to the same transaction where CMR was involved. &lt;/p&gt; &lt;h4&gt;xids: database table to save CMR processing data&lt;/h4&gt; &lt;p&gt; As said Narayana needs a special database table (usually named &lt;code&gt;xids&lt;/code&gt;) to save information that CMR was prepared. You may wonder what is content of that table. &lt;br/&gt; The table consists of three columns. &lt;ul&gt; &lt;li&gt;&lt;i&gt;xid&lt;/i&gt; : id of the transaction branch belonging to the CMR resource&lt;/li&gt; &lt;li&gt;&lt;i&gt;transactionManagerID&lt;/i&gt; : id of transaction manager, this serves to distinguish more transaction managers (WildFly servers) working with the same database. There is a strict rule that each transaction manager must be defined with unique transaction id (&lt;a href="https://wildscribe.github.io/WildFly/13.0/subsystem/transactions/index.html"&gt;see description of the node-identifer&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;i&gt;actionuid&lt;/i&gt; : global transaction id which unites all the resources belonging to the one particular transaction&lt;/li&gt; &lt;/ul&gt;&lt;/p&gt; &lt;h4&gt;LRCO failure case with CMR&lt;/h4&gt; &lt;p&gt; In the example, we presented as problematic for LRCO, the container crashed just before prepare phase finished. In such case, the CMR is not committed yet. The other transaction participants are then rolled-back as the transaction was not &lt;i&gt;fully prepared&lt;/i&gt;. The CMR brings the consistent rollback outcome for all the resources. &lt;/p&gt; &lt;h2&gt;Commit markable resource configured in WildFly&lt;/h2&gt; &lt;p&gt; We have sketched the principle of the CMR and now it's time to check how to configure it for your application running at the &lt;a href="http://wildfly.org"&gt;WildFly&lt;/a&gt; application server. &lt;br/&gt; The configuration consists of three steps. &lt;ol&gt; &lt;li&gt;The JDBC datasource needs to be marked as &lt;i&gt;connectable&lt;/i&gt;&lt;/li&gt; &lt;li&gt;The database, the connectable datasource points to, has to be enriched with the &lt;code&gt;xids&lt;/code&gt; table where Narayana can saves the data about CMR processing&lt;/li&gt; &lt;li&gt;Transaction subsystem needs to be configured to be aware of the CMR capable resource&lt;/li&gt; &lt;/ol&gt;&lt;/p&gt; &lt;p&gt; In our example, I use the H2 database as it's good for the showcase. You can find it in quickstart I prepared too. Check out the &lt;a href="https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource"&gt; https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource&lt;/a&gt;. &lt;/p&gt; &lt;h3&gt;Mark JDBC datasource as connectable&lt;/h3&gt; &lt;p&gt; You will mark the resource as &lt;code&gt;connectable&lt;/code&gt; when you use attribute &lt;code&gt;connectable="true"&lt;/code&gt; in your datasource declaration in &lt;code&gt;standalone*.xml&lt;/code&gt; configuration file. When you use jboss cli for the app server configuration you will use commands &lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;/subsystem=datasources/data-source=jdbc-cmr:write-attribute(name=connectable, value=true)&lt;br /&gt;:reload&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; The whole datasource configuration then looks like &lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&amp;lt;datasource jndi-name="java:jboss/datasources/jdbc-cmr" pool-name="jdbc-cmr-datasource"&lt;br /&gt; enabled="true" use-java-context="true" connectable="true"&amp;gt;&lt;br /&gt; &amp;lt;connection-url&amp;gt;jdbc:h2:mem:cmrdatasource&amp;lt;/connection-url&amp;gt;&lt;br /&gt; &amp;lt;driver&amp;gt;h2&amp;lt;/driver&amp;gt;&lt;br /&gt; &amp;lt;security&amp;gt;&lt;br /&gt; &amp;lt;user-name&amp;gt;sa&amp;lt;/user-name&amp;gt;&lt;br /&gt; &amp;lt;password&amp;gt;sa&amp;lt;/password&amp;gt;&lt;br /&gt; &amp;lt;/security&amp;gt;&lt;br /&gt;&amp;lt;/datasource&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; When datasource is marked as connectable then the IronJacamar (JCA layer of WildFly) creates the datasource instance as implementing &lt;a href="https://github.com/ochaloup/jboss-transaction-spi/blob/master/src/main/java/org/jboss/tm/ConnectableResource.java"&gt;&lt;code&gt;org.jboss.tm.ConnectableResource&lt;/code&gt;&lt;/a&gt; (defined in the &lt;a href="https://github.com/ochaloup/jboss-transaction-spi"&gt;jboss-transaction-spi project&lt;/a&gt;). This resource defines that the class provides method &lt;code&gt;getConnection() throws Throwable&lt;/code&gt;. That's how the transaction manager is capable to obtain the connection to the database and works with the &lt;code&gt;xids&lt;/code&gt; table inside it. &lt;/p&gt; &lt;h3&gt;Xids database table creation&lt;/h3&gt; &lt;p&gt; The database configured to be &lt;code&gt;connectable&lt;/code&gt; has to ensure existence of the &lt;code&gt;xids&lt;/code&gt; before transaction manager starts. As described above the &lt;code&gt;xids&lt;/code&gt; allows to save the cruical information about the non-XA datasource during prepare. The shape of the SQL command depends on the SQL syntax of the database you use. The example of the &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/resources/arjunacore/CommitMarkableResourceRecord.java#L74"&gt; table cleation commands is (see more commands under this link)&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code class="sql"&gt;-- Oracle&lt;br /&gt;CREATE TABLE xids (&lt;br /&gt; xid RAW(144), transactionManagerID VARCHAR(64), actionuid RAW(28)&lt;br /&gt;);&lt;br /&gt;CREATE UNIQUE INDEX index_xid ON xids (xid);&lt;br /&gt;&lt;br /&gt;-- PostgreSQL&lt;br /&gt;CREATE TABLE xids (&lt;br /&gt; xid bytea, transactionManagerID varchar(64), actionuid bytea&lt;br /&gt;);&lt;br /&gt;CREATE UNIQUE INDEX index_xid ON xids (xid);&lt;br /&gt;&lt;br /&gt;-- H2&lt;br /&gt;CREATE TABLE xids (&lt;br /&gt; xid VARBINARY(144), transactionManagerID VARCHAR(64), actionuid VARBINARY(28)&lt;br /&gt;);&lt;br /&gt;CREATE UNIQUE INDEX index_xid ON xids (xid);&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; I addressed the need of the table definition in &lt;a href="https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource"&gt;the CMR quickstart&lt;/a&gt; by adding the JPA schema generation create script which contains &lt;a href="https://github.com/jbosstm/quickstart/blob/master/wildfly/commit-markable-resource/src/main/resources/META-INF/persistence.xml#L44"&gt;the SQL to initialize the database&lt;/a&gt;. &lt;/p&gt; &lt;h3&gt;Transaction manager CMR configuration&lt;/h3&gt; &lt;p&gt; The last part is to configure the CMR for the transaction subsystem. The declaration puts the datasource under the list &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/jta/common/JTAEnvironmentBean.java#L104"&gt;JTAEnvironmentBean#commitMarkableResourceJNDINames&lt;/a&gt; which is then used in code of &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/transaction/arjunacore/TransactionImple.java#L798"&gt;TransactionImple#createResource&lt;/a&gt;. &lt;br/&gt; The xml element used in the transaction subsystem and the jboss cli commands look like &lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&amp;lt;commit-markable-resources&amp;gt;&lt;br /&gt; &amp;lt;commit-markable-resource jndi-name="java:jboss/datasources/jdbc-cmr"/&amp;gt;&lt;br /&gt;&amp;lt;/commit-markable-resources&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt;&lt;code class="bash"&gt;/subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr":add()&lt;br /&gt;:reload&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;h4&gt;CMR configuration options&lt;/h4&gt; &lt;p&gt; In addition to such simple CMR declaration, the CMR can be configured with following parameters &lt;/p&gt; &lt;p&gt; &lt;ul&gt; &lt;li&gt;&lt;b&gt;jndi-name&lt;/b&gt; : as it could be seen above the jndi-name is way to point to the datasource which we mark as CMR ready&lt;/li&gt; &lt;li&gt;&lt;b&gt;name&lt;/b&gt; : defines the name of the table which is used for storing the CMR state during prepare while used during recovery. &lt;br/&gt;The default value (and we've reffered to it in this way above) is &lt;code&gt;xids&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;b&gt;immediate-cleanup&lt;/b&gt; : If configured to true then there is registered &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/resources/arjunacore/CommitMarkableResourceRecord.java#L191"&gt;a synchronization&lt;/a&gt; which removes proper value from the &lt;code&gt;xids&lt;/code&gt; table immediatelly after the transaction is committed. &lt;br/&gt; When synchronization is not set up then the clean-up of the &lt;code&gt;xids&lt;/code&gt; table is responsibility of the recovery by the code at &lt;a href="https://github.com/jbosstm/narayana/blob/5.8.2.Final/ArjunaJTA/jta/classes/com/arjuna/ats/internal/jta/recovery/arjunacore/CommitMarkableResourceRecordRecoveryModule.java#L76"&gt;&lt;code&gt;CommitMarkableResourceRecordRecoveryModule&lt;/code&gt;&lt;/a&gt;. It checks about finished xids and it removes those which are free for garbage collection. &lt;br/&gt;The default value is &lt;code&gt;false&lt;/code&gt; (using only recovery garbage collection).&lt;/li&gt; &lt;li&gt;&lt;b&gt;batch-size&lt;/b&gt; : This parameter influences the process of the garbage collection (as described above). The garbage collection takes finished xids and runs &lt;code&gt;DELETE&lt;/code&gt; SQL command. The &lt;code&gt;DELETE&lt;/code&gt; contains the &lt;code&gt;WHERE xid in (...)&lt;/code&gt; clause with maximum of &lt;code&gt;batch-size&lt;/code&gt; entries provided. When there is still some finished xids left after deletion, another SQL command is assembled with maximum number of &lt;code&gt;batch-size&lt;/code&gt; entries again. &lt;br/&gt;The default value is &lt;code&gt;100&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;br/&gt; The &lt;code&gt;commit-markable-resource&lt;/code&gt; xml element configured with all the parameters looks like &lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&amp;lt;subsystem xmlns="urn:jboss:domain:transactions:4.0"&amp;gt;&lt;br /&gt; &amp;lt;core-environment&amp;gt;&lt;br /&gt; &amp;lt;process-id&amp;gt;&lt;br /&gt; &amp;lt;uuid/&amp;gt;&lt;br /&gt; &amp;lt;/process-id&amp;gt;&lt;br /&gt; &amp;lt;/core-environment&amp;gt;&lt;br /&gt; &amp;lt;recovery-environment socket-binding="txn-recovery-environment" status-socket-binding="txn-status-manager"/&amp;gt;&lt;br /&gt; &amp;lt;object-store path="tx-object-store" relative-to="jboss.server.data.dir"/&amp;gt;&lt;br /&gt; &amp;lt;commit-markable-resources&amp;gt;&lt;br /&gt; &amp;lt;commit-markable-resource jndi-name="java:jboss/datasources/jdbc-cmr"&amp;gt;&lt;br /&gt; &amp;lt;xid-location name="myxidstable" batch-size="10" immediate-cleanup="true"/&amp;gt;&lt;br /&gt; &amp;lt;/commit-markable-resource&amp;gt;&lt;br /&gt; &amp;lt;/commit-markable-resources&amp;gt;&lt;br /&gt;&amp;lt;/subsystem&amp;gt;&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And the jboss cli commands for the same are&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;/subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr"\&lt;br /&gt; :write-attribute(name=name, value=myxidstable)&lt;br /&gt; /subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr"\&lt;br /&gt; :write-attribute(name=immediate-cleanup, value=true)&lt;br /&gt;/subsystem=transactions/commit-markable-resource="java:jboss/datasources/jdbc-cmr"\&lt;br /&gt; :write-attribute(name=batch-size, value=10)&lt;br /&gt;:reload&lt;br /&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt; &lt;i&gt;NOTE:&lt;/i&gt; the JBoss EAP documentation about the CMR resource configuration can be found at section &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_jboss_enterprise_application_platform/7.1/html/development_guide/java_transaction_api_jta#about_the_lrco_optimization_for_single_phase_commit_1pc"&gt; About the LRCO Optimization for Single-phase Commit (1PC)&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt; The article explained what is the Narayana Commit Markable resource (CMR), it compared it with LRCO and presented its advantages. In the latter part of the article you found how to configure the CMR resource in your application deployed at the &lt;a href="http://wildfly.org/"&gt;WildFly application server&lt;/a&gt;. &lt;br/&gt; If you like to run an application using the commit markable resource feature, check our Narayana quickstart at &lt;a href="https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource"&gt; &lt;b&gt;https://github.com/jbosstm/quickstart/tree/master/wildfly/commit-markable-resource&lt;/b&gt;&lt;/a&gt;. &lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qq44hyr6kbA" height="1" width="1" alt=""/&gt;</content><summary>CMR is neat Narayana feature enabling full XA transaction capability for one non-XA JDBC resource. This gives you a way to engage a database resource to XA transaction even the JDBC driver is not fully XA capable (or you just have a design restriction on it) while transaction data consistency is kept. Last resource commit optimization (aka. LRCO) Maybe you will say "adding one non-XA resource to a...</summary><dc:creator>Ondřej Chaloupka</dc:creator><dc:date>2018-06-28T05:17:00Z</dc:date><feedburner:origLink>http://jbossts.blogspot.com/2018/06/narayana-commit-markable-resource.html</feedburner:origLink></entry><entry><title>Building Syndesis platform with Apache Camel snapshot</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_kktvrw9pE8/" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_oscerd" scheme="searchisko:content:tags" /><author><name>Andrea Cosentino</name></author><id>searchisko:content:id:jbossorg_blog-building_syndesis_platform_with_apache_camel_snapshot</id><updated>2018-06-28T00:00:00Z</updated><published>2018-06-28T00:00:00Z</published><content type="html">&lt;p&gt;In the last months I worked on &lt;a href="https://syndesis.io/"&gt;Syndesis&lt;/a&gt; project. Syndesis is an hybrid integration platform based on Apache Camel. During this time I had the need to build this platform against a Camel Snapshot version to test some new features I added into the Apache Camel project and it wasn’t truly easy. Adding the possibility to build the platform against different Camel snapshots and versions can be very useful to test Camel master and also to have an idea of how new/updated Camel components behave in this platform. I think it would be useful for end users too.&lt;/p&gt; &lt;h3 id="normal-workflow"&gt;Normal Workflow&lt;/h3&gt; &lt;p&gt;Building Syndesis platform is not super easy at first sight, but it’s very well documented and complete.&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis build &lt;span class="nt"&gt;--help&lt;/span&gt; Run Syndesis builds Usage: syndesis build &lt;span class="o"&gt;[&lt;/span&gt;... options ...] Options &lt;span class="k"&gt;for &lt;/span&gt;build: &lt;span class="nt"&gt;-b&lt;/span&gt; &lt;span class="nt"&gt;--backend&lt;/span&gt; Build only backend modules &lt;span class="o"&gt;(&lt;/span&gt;core, extension, integration, connectors, server, meta&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;--images&lt;/span&gt; Build only modules with Docker images &lt;span class="o"&gt;(&lt;/span&gt;ui, server, meta, s2i&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;-m&lt;/span&gt; &lt;span class="nt"&gt;--module&lt;/span&gt; &amp;lt;m1&amp;gt;,&amp;lt;m2&amp;gt;, .. Build modules Modules: ui, server, connector, s2i, meta, integration, extension, common &lt;span class="nt"&gt;-d&lt;/span&gt; &lt;span class="nt"&gt;--dependencies&lt;/span&gt; Build also all project the specified module depends on &lt;span class="nt"&gt;--skip-tests&lt;/span&gt; Skip unit and system &lt;span class="nb"&gt;test &lt;/span&gt;execution &lt;span class="nt"&gt;--skip-checks&lt;/span&gt; Disable all checks &lt;span class="nt"&gt;-f&lt;/span&gt; &lt;span class="nt"&gt;--flash&lt;/span&gt; Skip checks and tests execution &lt;span class="o"&gt;(&lt;/span&gt;fastest mode&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;-i&lt;/span&gt; &lt;span class="nt"&gt;--image-mode&lt;/span&gt; &amp;lt;mode&amp;gt; &amp;lt;mode&amp;gt; can be - &lt;span class="s2"&gt;"none"&lt;/span&gt; : No images are build &lt;span class="o"&gt;(&lt;/span&gt;default&lt;span class="o"&gt;)&lt;/span&gt; - &lt;span class="s2"&gt;"openshift"&lt;/span&gt; : Build &lt;span class="k"&gt;for &lt;/span&gt;OpenShift image streams - &lt;span class="s2"&gt;"docker"&lt;/span&gt; : Build against a plain Docker daemon - &lt;span class="s2"&gt;"auto"&lt;/span&gt; : Automatically detect whether to use &lt;span class="s2"&gt;"openshift"&lt;/span&gt; or &lt;span class="s2"&gt;"docker"&lt;/span&gt; &lt;span class="nt"&gt;--docker&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nt"&gt;--image-mode&lt;/span&gt; docker &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nt"&gt;--image-mode&lt;/span&gt; openshift &lt;span class="nt"&gt;-p&lt;/span&gt; &lt;span class="nt"&gt;--project&lt;/span&gt; &amp;lt;project&amp;gt; Specifies the project to create images &lt;span class="k"&gt;in &lt;/span&gt;when using &lt;span class="s1"&gt;'--openshift'&lt;/span&gt; &lt;span class="nt"&gt;-k&lt;/span&gt; &lt;span class="nt"&gt;--kill-pods&lt;/span&gt; Kill pods after the image has been created. Useful when building with image-mode docker &lt;span class="nt"&gt;-c&lt;/span&gt; &lt;span class="nt"&gt;--clean&lt;/span&gt; Run clean builds &lt;span class="o"&gt;(&lt;/span&gt;mvn clean&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="nt"&gt;--batch-mode&lt;/span&gt; Run mvn &lt;span class="k"&gt;in &lt;/span&gt;batch mode &lt;span class="nt"&gt;--camel-snapshot&lt;/span&gt; Run a build with a specific Camel snapshot. You&lt;span class="s1"&gt;'ll need to set an environment variable CAMEL_SNAPSHOT_VERSION with the SNAPSHOT version you want to use. --man Open HTML documentation in the Syndesis Developer Handbook &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;I use &lt;a href="https://github.com/minishift/minishift"&gt;Minishift&lt;/a&gt; to play with Syndesis and my normal workflow is the following: First I spin up a Minishift instance&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; minishift start &lt;span class="nt"&gt;--memory&lt;/span&gt; 8384 &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;You could need to specify a vm-driver too with the –vm-driver flag.&lt;/p&gt; &lt;p&gt;Then I set the docker environment coming from Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;eval&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;minishift docker-env&lt;span class="k"&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;At this point I’m able to build&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis build &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;span class="nt"&gt;--clean&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Once the build it’s done (it may take a while) we are able to deploy Syndesis platform on Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis minishift &lt;span class="nt"&gt;--install&lt;/span&gt; &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Once the deployment is finished we are able to start using Syndesis platform.&lt;/p&gt; &lt;h3 id="building-with-a-camel-snapshot"&gt;Building with a Camel Snapshot&lt;/h3&gt; &lt;p&gt;The option you’ll need in this case will be –camel-snapshot, in combination with an environment variable called &lt;code class="highlighter-rouge"&gt;CAMEL_SNAPSHOT_VERSION&lt;/code&gt;. In my case I need to test a new feature in a component from Camel 2.21.2-SNAPSHOT. The workflow to obtain a running Syndesis instance based on Camel 2.21.2-SNAPSHOT is the following (supposing you have a running Minishift).&lt;/p&gt; &lt;p&gt;Set the docker environment coming from Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;eval&lt;/span&gt; &lt;span class="k"&gt;$(&lt;/span&gt;minishift docker-env&lt;span class="k"&gt;)&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Export &lt;code class="highlighter-rouge"&gt;CAMEL_SNAPSHOT_VERSION&lt;/code&gt; environment variable&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;CAMEL_SNAPSHOT_VERSION&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;"2.21.2-SNAPSHOT"&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Run a Syndesis build with –camel-snapshot flag enabled&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis build &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;span class="nt"&gt;--clean&lt;/span&gt; &lt;span class="nt"&gt;--camel-snapshot&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;Once the build finished, you can deploy your Syndesis on Minishift&lt;/p&gt; &lt;div class="language-bash highlighter-rouge"&gt;&lt;div class="highlight"&gt;&lt;pre class="highlight"&gt;&lt;code&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; syndesis/tools/bin&lt;span class="nv"&gt;$ &lt;/span&gt;./syndesis minishift &lt;span class="nt"&gt;--install&lt;/span&gt; &lt;span class="nt"&gt;--openshift&lt;/span&gt; &lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt; &lt;p&gt;This is all you need to test a Syndesis platform based on Camel snapshot.&lt;/p&gt; &lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt; &lt;p&gt;In the future I’ll blog about the Syndesis platform a bit more. If you want to contribute you can start from the &lt;a href="https://github.com/syndesisio/syndesis/"&gt;Github project&lt;/a&gt;, the &lt;a href="https://syndesis.io/"&gt;site&lt;/a&gt; or an &lt;a href="https://github.com/syndesisio/syndesis-extensions"&gt;extension&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_kktvrw9pE8" height="1" width="1" alt=""/&gt;</content><summary>In the last months I worked on Syndesis project. Syndesis is an hybrid integration platform based on Apache Camel. During this time I had the need to build this platform against a Camel Snapshot version to test some new features I added into the Apache Camel project and it wasn’t truly easy. Adding the possibility to build the platform against different Camel snapshots and versions can be very use...</summary><dc:creator>Andrea Cosentino</dc:creator><dc:date>2018-06-28T00:00:00Z</dc:date><feedburner:origLink>http://oscerd.github.io/2018/06/28/building-syndesis-with-camel-snapshot/</feedburner:origLink></entry><entry><title>DesOps is “DevOps 2.0”</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/JU5Mi4El6_w/" /><category term="DevOps" /><category term="UI/UX" /><category term="Agile" /><category term="design" /><category term="DesignOps" /><category term="DesOPs" /><category term="domain-driven design" /><category term="SDLC" /><author><name>Samir Dash</name></author><id>https://developers.redhat.com/blog/?p=503557</id><updated>2018-06-27T21:32:36Z</updated><published>2018-06-27T21:32:36Z</published><content type="html">&lt;p&gt;As we discussed in the &lt;a href="https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/"&gt;last post&lt;/a&gt;, most of &lt;em&gt;DevOps&lt;/em&gt; today focuses on the process blocks that mostly impact engineering or technical aspects of a product rather than the design aspect. Even though &lt;em&gt; DesOps&lt;/em&gt; was primarily born out of the primary need of how to design at scale, the factors that shaped it are of a similar nature to the factors that shaped &lt;em&gt;DevOps&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;With recent software delivery processes, for example, the Agile process and Continuous Integration (CI) and Continuous Deployment (CD)of code, the &lt;em&gt;DevOps&lt;/em&gt; approach provided a faster highway to ensure faster delivery with low risks. So the earlier SDLC model got redefined over time with Agile and then with &lt;em&gt;DevOps&lt;/em&gt; to its current shape.&lt;/p&gt; &lt;p&gt;However, because design is an integral part of any product delivered, there is a need to ensure that gaps are bridged between the traditional design lifecycle and the fast track of the &lt;em&gt;DevOps&lt;/em&gt; development lifecycle. &lt;em&gt;DesOps&lt;/em&gt; and &lt;em&gt;DevOps&lt;/em&gt; both are complementary to each other. The design delivery process improvements try to optimize the overall delivery process and thereby contribute to &lt;em&gt;DevOps&lt;/em&gt;, for example, in aspects such as testing of the product that involves design aspects, usability, accessibility, etc.&lt;/p&gt; &lt;p&gt;The need for tighter integration between the design team and the engineering team became a necessity to ensure to design at scale. During the past two to three years, the top five big companies have made heavy investments in this area that have paved the way for other organizations and design communities to be more explorative in this area.&lt;/p&gt; &lt;p&gt;&lt;span id="more-503557"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;img class="wp-image-503577 size-full aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0.png" alt="DesOps = DevOps 2.0" width="1492" height="708" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0.png 1492w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0-300x142.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0-768x364.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2.0-1024x486.png 1024w" sizes="(max-width: 1492px) 100vw, 1492px" /&gt;&lt;/p&gt; &lt;p&gt;The implications of &lt;em&gt;DesOps&lt;/em&gt; are reflected in the outcome, where the silos among the teams and disciplines get reduced. Along with this, &lt;em&gt;DesOps&lt;/em&gt; improves the collaboration among cross-functional teams and work practices, which contributes to minimizing waste in the delivery process.&lt;/p&gt; &lt;p&gt;Every product lifecycle has one core goal towards which it strives: reaching customer delight by delivering the value. The design process associated with &lt;em&gt;DesOps&lt;/em&gt; helps in understanding, capturing, and delivering that value.&lt;/p&gt; &lt;p&gt;But conventional business processes were more keen on getting the outputs of each process block, which can be fed into the next block, thereby reaching a stage that ultimately delivered the value. Many such practices failed in achieving customer delight because the processes used were not based on the customer shift from outputs to outcomes.&lt;/p&gt; &lt;p&gt;If we see the &lt;em&gt;DesOps&lt;/em&gt; processes in terms of a value system, we will see at a high level that &lt;em&gt;DesOps&lt;/em&gt; touches upon three major areas:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Understanding value&lt;/li&gt; &lt;li&gt;Creating value&lt;/li&gt; &lt;li&gt;Capturing and delivering value&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;As discussed in the last post, in the value system approach,&lt;em&gt; understanding value&lt;/em&gt; is about reaching a vision. &lt;em&gt;Creating value&lt;/em&gt; is broadly about reaching a roadmap with &lt;em&gt;Minimum Viable Product&lt;/em&gt; (MVP). &lt;em&gt;Capturing and delivery value &lt;/em&gt;is about running the backlog and sprints and ensuring delivery until the end users have access.&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;&lt;img class="alignnone size-full wp-image-503607" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap.png" alt="" width="1423" height="704" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap.png 1423w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap-300x148.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap-768x380.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desopsdevops2-overlap-1024x507.png 1024w" sizes="(max-width: 1423px) 100vw, 1423px" /&gt;&lt;/p&gt; &lt;p&gt;&amp;#160;&lt;/p&gt; &lt;p&gt;Because design is associated with entities and attributes beyond the quantifiable science of connecting with disciplines that are associated with the emotional aspects of the qualitative approach, the implementation of &lt;em&gt;DesOps&lt;/em&gt; is more fluid than the case of &lt;em&gt;DevOps&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;For instance, if we plot the techniques and practices of any domain according to the left brain–right brain analogy, we can see that most of the practices having soft-attributes or dealing with human emotions, purpose, and behavior will fall into the creative aspect, which involves some kind of design approach to problem-solving.&lt;/p&gt; &lt;p&gt;As one example, you can see in the following figure that the right side shows the mapping of practices involved with software incident reporting. Here you can easily notice the pattern.&lt;/p&gt; &lt;p&gt;&lt;img class="size-full wp-image-503797 aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright.png" alt="" width="1224" height="803" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright.png 1224w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright-300x197.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright-768x504.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/leftright-1024x672.png 1024w" sizes="(max-width: 1224px) 100vw, 1224px" /&gt;&lt;/p&gt; &lt;p class="selectionShareable"&gt;The typical belief is that the left side of the brain mostly processes logical thinking, while the right side is more about emotional thinking. Based on this popular analogy, when we map across a straight line from left to right (refer to the previous diagram) the different aspects involved in different stages of SDLC for a digital product, we will notice the logical and more human-centered aspects are divided by an imaginary line from the center. We will also notice the gradual progression of the emotional index for the components from left to right. This helps to demonstrate how the more-human angle is involved as we move from the areas that DevOps touches upon to the areas that &lt;em&gt;DesOps&lt;/em&gt; touches, because &lt;em&gt;DesOps&lt;/em&gt; touches upon the design and business aspects of the value chain.&lt;/p&gt; &lt;p&gt;From foundational guidelines, &lt;em&gt;DevOps&lt;/em&gt; inspires the &lt;em&gt;DesOps&lt;/em&gt; mindset at a high level:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Partnering with customers for improving business value&lt;/li&gt; &lt;li&gt;Working together towards a shared vision&lt;/li&gt; &lt;li&gt;Delivering incremental value&lt;/li&gt; &lt;li&gt;Investing in quality&lt;/li&gt; &lt;li&gt; Empowering team members&lt;/li&gt; &lt;li&gt;Setting up clear accountability in teams&lt;/li&gt; &lt;li&gt;Learning from experiences&lt;/li&gt; &lt;li&gt; Advocating open communications and transparency&lt;/li&gt; &lt;li&gt;Being agile and adapting to change&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In such a context, if we look into &lt;em&gt;DesOps&lt;/em&gt;, it makes a lot of sense in the following key principles:&lt;/p&gt; &lt;p&gt;&lt;img class="size-full wp-image-503807 aligncenter" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222.png" alt="" width="1920" height="1329" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222.png 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222-300x208.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222-768x532.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops222-1024x709.png 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Implement &lt;em&gt;DesOps&lt;/em&gt; to follow service design methodologies.&lt;/li&gt; &lt;li&gt;The feedback loop should cut across the product lifecycle.&lt;/li&gt; &lt;li&gt;Empower stakeholders for better decision-making—hypothesis and data-driven decision-making for design and development.&lt;/li&gt; &lt;li&gt;Empower design thinking.&lt;/li&gt; &lt;li&gt;Advocate lean methodologies and Agile philosophies.&lt;/li&gt; &lt;li&gt;Translate user-centered design into an actual process that can be used on the ground.&lt;/li&gt; &lt;li&gt;Advocate cohesive designers, stakeholders, and developers into team play.&lt;/li&gt; &lt;li&gt;Technology decisions should be guided by lowering the boundaries between roles and automation to reduce waste and reduce repetitive jobs to work for the product and the project.&lt;/li&gt; &lt;li&gt;Redesign and re-engineer the processes.&lt;/li&gt; &lt;li&gt;Enable reviews based on data-driven benchmarks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So how does this all fit in? At a crude level, the &lt;em&gt;DesOps&lt;/em&gt; processes may have something like the following structure when they are translated in terms of technology and ecosystems, in a similar fashion as DevOps.&lt;/p&gt; &lt;p&gt;&lt;img class="aligncenter size-full wp-image-503827" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2.png" alt="" width="1500" height="323" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2.png 1500w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2-300x65.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2-768x165.png 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/flow2-1024x221.png 1024w" sizes="(max-width: 1500px) 100vw, 1500px" /&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First, we should create the design benchmarks (that includes qualitative as well as quantitative metrics) from the information at the design stage that can be used in comparing the product features against metrics based on this design benchmark.&lt;/li&gt; &lt;li&gt;Then, automate and perform manual tracking of the product during runtime (in real time and in the true context), and then categorize and collate this data.&lt;/li&gt; &lt;li&gt;This involves creating features to support the user feedback cycle and user testing aspects (exploratory, split testing capabilities).&lt;/li&gt; &lt;li&gt;Collect all standards and specifications on different aspects of heuristics to ensure that at least at the basic level the standard principles are followed.&lt;/li&gt; &lt;li&gt;On the ground, in the context of the eco-system and technologies, build the critical components that would collect and process all the data collected in all these stages and generate the desired metrics and inferences and also contribute in bringing continuous integration and continuous delivery blocks to run the process.&lt;/li&gt; &lt;li&gt;Build the unit to generate the model to map the data and compare it against the metrics.&lt;/li&gt; &lt;li&gt;Build the cognitive unit that can compare the data and apply the correct models and metrics to carry out the filtering of the data and generate the insights which can be shared as actionable output to the end-user/customer.&lt;/li&gt; &lt;li&gt;And ensure in all these stages that the feedback loop is connected spatially and acting as a meaningful neural network that helps in informed decision-making.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keep tuned in to stay with me on this journey into &lt;em&gt;DesOps&lt;/em&gt;!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;(Note: Based on my book &lt;em&gt;The DesOps Enterprise: Overview &amp;#38; Culture&lt;/em&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;linkname=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F27%2Fdesops-is-devops-2-0%2F&amp;#38;title=DesOps%20is%20%E2%80%9CDevOps%202.0%E2%80%9D" data-a2a-url="https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/" data-a2a-title="DesOps is “DevOps 2.0”"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/"&gt;DesOps is &amp;#8220;DevOps 2.0&amp;#8221;&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/JU5Mi4El6_w" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;As we discussed in the last post, most of DevOps today focuses on the process blocks that mostly impact engineering or technical aspects of a product rather than the design aspect. Even though  DesOps was primarily born out of the primary need of how to design at scale, the factors that shaped it are of a similar nature [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/"&gt;DesOps is &amp;#8220;DevOps 2.0&amp;#8221;&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">503557</post-id><dc:creator>Samir Dash</dc:creator><dc:date>2018-06-27T21:32:36Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/27/desops-is-devops-2-0/</feedburner:origLink></entry><entry><title>Keycloak on Kubernetes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/-EIEUoVLx2s/keycloak-on-kubernetes.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Stian Thorgersen</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_on_kubernetes</id><updated>2018-06-27T20:15:37Z</updated><published>2018-06-27T20:15:00Z</published><content type="html">&lt;p&gt;If you'd like to get started with using Keycloak on Kubernetes check out &lt;a href="https://youtu.be/A_BYZ7hHWXE"&gt;this screencast&lt;/a&gt;. If you'd rather try it out yourself check out &lt;a href="https://github.com/stianst/demo-kubernetes"&gt;this GitHub repository&lt;/a&gt; that contains the instructions as well as all the bits you'll need to reproduce what is shown in the screencast.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/-EIEUoVLx2s" height="1" width="1" alt=""/&gt;</content><summary>If you'd like to get started with using Keycloak on Kubernetes check out this screencast. If you'd rather try it out yourself check out this GitHub repository that contains the instructions as well as all the bits you'll need to reproduce what is shown in the screencast.</summary><dc:creator>Stian Thorgersen</dc:creator><dc:date>2018-06-27T20:15:00Z</dc:date><feedburner:origLink>http://blog.keycloak.org/2018/06/keycloak-on-kubernetes.html</feedburner:origLink></entry><entry><title>Making Java objects queryable by Infinispan remote clients</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/i3zXH1Mdwcw/making-java-objects-queryable-by.html" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="remote query" scheme="searchisko:content:tags" /><author><name>Galder Zamarreño</name></author><id>searchisko:content:id:jbossorg_blog-making_java_objects_queryable_by_infinispan_remote_clients</id><updated>2018-06-27T11:47:22Z</updated><published>2018-06-27T11:46:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;The following is a common question amongst Infinispan community users:&lt;br /&gt;&lt;blockquote class="tr_bq"&gt;&lt;b&gt;How do I make my Java objects queryable by remote clients?&lt;/b&gt;&amp;nbsp;&lt;/blockquote&gt;&lt;div style="text-align: left;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;h2 style="text-align: left;"&gt;Annotation Method&lt;/h2&gt;&lt;br /&gt;The simplest way is to take advantage &lt;a href="https://github.com/infinispan/protostream"&gt;Infinispan Protostream&lt;/a&gt; annotations to mark your objects queryable and decide how each object field should be indexed. Example:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/6ba1e043b6ab7f8c5e0971500d30ff12.js"&gt;&lt;/script&gt; Then, the &lt;a href="https://github.com/infinispan/protostream/blob/master/core/src/main/java/org/infinispan/protostream/annotations/ProtoSchemaBuilder.java"&gt;ProtoSchemaBuilder&lt;/a&gt; can inspect the annotated class and derive a &lt;a href="https://developers.google.com/protocol-buffers/"&gt;Google Protocol Buffers&lt;/a&gt; schema file from it. Example:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/66b2b60305bd1f5d4e7762b4c21b366a.js"&gt;&lt;/script&gt; Finally, the schema file needs to be registered in the “&lt;span style="font-family: &amp;quot;courier new&amp;quot; , &amp;quot;courier&amp;quot; , monospace;"&gt;___protobuf_metadata&lt;/span&gt;” cache:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/da67bac08b3e5e6aa02da26f76821a50.js"&gt;&lt;/script&gt; Although this is by far the easiest way to make your Java objects queryable, this method might not always be viable. For example, you might not be able to modify the Java object classes to add the annotations. For such use cases, a more verbose method is available that does not require modifying the source code of the Java object.&lt;br /&gt;&lt;br /&gt;&lt;h2 style="text-align: left;"&gt;Plain Object Method&lt;/h2&gt;&lt;br /&gt;For example, given this Java object:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/b217b71a47ebd16a1fcb8f1ddb744c7a.js"&gt;&lt;/script&gt; A Protocol Buffers schema must be defined where comments are used to define the object as queryable and decide how each field is indexed:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/8921fd5325305e7f6a5d980523b6f5be.js"&gt;&lt;/script&gt; This method also requires a &lt;a href="https://github.com/infinispan/protostream/blob/master/core/src/main/java/org/infinispan/protostream/MessageMarshaller.java"&gt;Protostream message marshaller&lt;/a&gt; to be defined which specifies how each field is serialized/deserialized:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/4170abb6dd522ac3250989d8e842ab90.js"&gt;&lt;/script&gt; This method still requires the Protocol Buffers schema to be registered remotely, but on top of that, the schema and marshaller need to be registered in the client:&lt;br /&gt;&lt;br /&gt;&lt;script src="https://gist.github.com/galderz/debff7f6d84a942183dc4b879dbf28c1.js"&gt;&lt;/script&gt; Clearly, this second method is a lot more verbose and more laborious when refactoring. If any changes are made to the Java object, the marshaller and Protocol Buffer schema need to also be changed accordingly. This is done automatically in the first method.&lt;br /&gt;&lt;br /&gt;Both methods are demonstrated in full in the &lt;a href="https://github.com/infinispan-demos/queryable-pojos"&gt;queryable-pojos demo&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Cheers&lt;br /&gt;Galder&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/Infinispan/~4/zMYxbORR2AE" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/i3zXH1Mdwcw" height="1" width="1" alt=""/&gt;</content><summary>The following is a common question amongst Infinispan community users: How do I make my Java objects queryable by remote clients?  Annotation Method The simplest way is to take advantage Infinispan Protostream annotations to mark your objects queryable and decide how each object field should be indexed. Example: Then, the ProtoSchemaBuilder can inspect the annotated class and derive a Google Proto...</summary><dc:creator>Galder Zamarreño</dc:creator><dc:date>2018-06-27T11:46:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/Infinispan/~3/zMYxbORR2AE/making-java-objects-queryable-by.html</feedburner:origLink></entry><entry><title>Using Red Hat Data Grid to power a multi-cloud real-time game</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xt37-YNpkAI/" /><category term="Red Hat JBoss Data Grid" /><category term="Red Hat Summit" /><category term="Data Grid" /><category term="demo" /><category term="Infinispan" /><category term="JBoss Data Grid" /><category term="OpenShift Container Platform" /><category term="Red Hat Data Grid" /><category term="Red Hat OpenShift" /><category term="red hat summit" /><category term="Red Hat Summit 2018" /><author><name>Galder Zamarreno</name></author><id>https://developers.redhat.com/blog/?p=502937</id><updated>2018-06-26T11:00:30Z</updated><published>2018-06-26T11:00:30Z</published><content type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/blog/2018/05/10/red-hat-summit-2018-burr-sutter-demo/"&gt;scavenger hunt game&lt;/a&gt; developed for the audience to play during the Red Hat Summit 2018 demo used Red Hat Data Grid as storage for everything except the pictures taken by the participants. Data was stored &lt;a href="https://developers.redhat.com/blog/2018/06/19/red-hat-data-grid-on-three-clouds/"&gt;across three different cloud environments using cross-site replication&lt;/a&gt;. In this blog post, we will look at how data was flowing through Data Grid and explain the Data Grid features powering different aspects of the game&amp;#8217;s functionality.&lt;/p&gt; &lt;p&gt;In its simplest form, Data Grid exposes a key/value map API. This API was used throughout the game for storing and retrieving information such as game tasks, per-site active users, players, picture/task attempts (also called transactions), and scores.&lt;/p&gt; &lt;p&gt;Some of the data, such as player information including each player’s score was indexed for speed of retrieval. This was done in order to easily calculate the leaderboard, that is, the top 10 players with the highest score. So, from the player’s data, we only needed to index its score. We were able to do that very easily using Infinispan Protostream annotations (full source code can be found &lt;a href="https://github.com/rhdemo/scavenger/blob/master/microservices/scavenger-microservice/src/main/java/me/escoffier/keynote/Player.java"&gt;here&lt;/a&gt;), example:&lt;/p&gt; &lt;pre&gt;@ProtoDoc("@Indexed") @ProtoMessage(name = "Player") public class Player { private int score; … @ProtoDoc("@IndexedField") @ProtoField(number = 10, required = true) public int getScore() { return score; } ... } &lt;/pre&gt; &lt;p&gt;Calculating the leaderboard was easy. The Data Grid client simply had to build the query for it, send it to the Data Grid and handle results. The query looked something like this:&lt;/p&gt; &lt;pre&gt;Query query = queryFactory.from(Player.class) .orderBy("score", SortOrder.DESC) .maxResults(10) .build(); &lt;/pre&gt; &lt;p&gt;It’s worth noting that both the player data and the index itself were replicated across sites. This meant that no matter which cloud you’d hit, you’d get the same, consistent result.&lt;/p&gt; &lt;p&gt;Even though it didn’t make the final cut, we had also planned for users to be able to figure out their individual position in the leaderboard. Coming up with such a query was a bit more tricky, but we could achieve that by using this query:&lt;/p&gt; &lt;pre&gt;int playerScore = ... Query query = qf.from(Player.class) .select(count("score")) .orderBy("score", SortOrder.DESC) .having("score").gt(playerScore) .groupBy("score") .build(); List list = query.list(); final long playerRank = list.size() + 1; &lt;/pre&gt; &lt;p&gt;The query groups the number of different scores, sort them in descending order and count how many are bigger than a given player’s score. The number returned, plus 1, would give us the players position. This meant that if several players had the same score, they’d all share the same position but this is quite common, e.g. golf tournament rankings.&lt;/p&gt; &lt;p&gt;Keeping indexed data across different clouds was not always easy. One issue we had to deal with is the lack of default cross-site replication for the indexed data schema. To be able to index data, Data Grid must be able to decompose binary data received from the client to discover individual fields, figure out which ones to index, etc. To solve this problem, we created a schema keeper component (code can be found &lt;a href="https://github.com/rhdemo/scavenger-schemaer"&gt;here&lt;/a&gt;) which checked if the player’s schema was present in a given cloud, and if it wasn’t, register it. This component was deployed as a sidecar with each of the Data Grid instances. The Data Grid team is working to avoid the need for such component in the future.&lt;/p&gt; &lt;p&gt;Whenever a picture was uploaded and it was scored, the score would be stored individually inside Data Grid. This would trigger the Data Grid to send an event using remote client listeners, which would be picked by the game and would forward it to the user. Bearing in mind that Data Grid stores data in a key/value pair structure, the score would be stored in the value part. However, by default remote client listener events only ship key (and version) information. To avoid an extra lookup, the remote client listener was configured with a converter factory named &lt;code&gt;key-value-with-previous-converter-factory&lt;/code&gt; which is available out-of-the-box. By doing this, each event was transformed to contain the value part as well as the key. Example:&lt;/p&gt; &lt;pre&gt;@ClientListener(converterFactoryName = "key-value-with-previous-converter-factory", ...) public class RemoteCacheListener { ... }&lt;/pre&gt; &lt;p&gt;For each new score, we wanted a single event to be fired. However, due to how remote client listeners work, each score would, by default, fire as many events as different cloud/sites available. To avoid this, we used remote client listener filters to create a server-side deployed filter which would compare the score’s cloud of origin (AWS, Azure or Private) with the cloud where the filter was being executed.&lt;/p&gt; &lt;p&gt;For example, if a score originated in AWS, only the filter running inside the AWS cloud would allow the event to be fired to the client. When this score arrived in Azure or Private clouds, the filter would detect the event originated in a different cloud and would not fire it. The code for the filter can be found &lt;a href="https://github.com/rhdemo/infinispan-listener-optimizations/blob/master/src/main/java/fn/dg/os/filters/SiteFilterFactory.java"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To add it to the server, the filter needs to be deployed into it. For the demo, we extended the base Data Grid image with JAR containing the filter and related files.&lt;/p&gt; &lt;p&gt;Finally, to apply the filter to the listener, we added the name of the filter factory to the &lt;code&gt;filterFactoryName&lt;/code&gt; property of the client listener annotation. Example:&lt;/p&gt; &lt;pre&gt;@ClientListener( converterFactoryName = "key-value-with-previous-converter-factory", filterFactoryName = "site-filter-factory" ) public class RemoteCacheListener { … }&lt;/pre&gt; &lt;p&gt;This concludes this blog post where we looked at how the scavenger hunt game’s application layer used Red Had Data Grid to store and expose metadata information used throughout the game.&lt;/p&gt; &lt;p&gt;The Red Hat Data Grid Team&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;linkname=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F26%2Fdata-grid-multi-cloud-real-time-game%2F&amp;#38;title=Using%20Red%20Hat%20Data%20Grid%20to%20power%20a%20multi-cloud%20real-time%20game" data-a2a-url="https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/" data-a2a-title="Using Red Hat Data Grid to power a multi-cloud real-time game"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/"&gt;Using Red Hat Data Grid to power a multi-cloud real-time game&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xt37-YNpkAI" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;The scavenger hunt game developed for the audience to play during the Red Hat Summit 2018 demo used Red Hat Data Grid as storage for everything except the pictures taken by the participants. Data was stored across three different cloud environments using cross-site replication. In this blog post, we will look at how data was [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/"&gt;Using Red Hat Data Grid to power a multi-cloud real-time game&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">502937</post-id><dc:creator>Galder Zamarreno</dc:creator><dc:date>2018-06-26T11:00:30Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/26/data-grid-multi-cloud-real-time-game/</feedburner:origLink></entry><entry><title>Infinispan 9.3.0.Final is out!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rvx4rV1zi7U/infinispan-930final-is-out.html" /><category term="9.3" scheme="searchisko:content:tags" /><category term="feed_group_name_infinispan" scheme="searchisko:content:tags" /><category term="feed_name_infinispan" scheme="searchisko:content:tags" /><category term="final" scheme="searchisko:content:tags" /><category term="release" scheme="searchisko:content:tags" /><author><name>Galder Zamarreño</name></author><id>searchisko:content:id:jbossorg_blog-infinispan_9_3_0_final_is_out</id><updated>2018-06-26T07:12:29Z</updated><published>2018-06-26T07:12:00Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;We're delighted to announce the release of Infinispan 9.3.0.Final, which is a culmination of several months of hard work by the entire Infinispan community. Here's a summary of what you can find within it:&lt;br /&gt;&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;First final release to work with both Java 8 and Java 10. Note that Infinispan only works in classpath mode.&lt;/li&gt;&lt;li&gt;Transaction support Hot Rod.&amp;nbsp;The java Hot Rod client can participate in Java transactions via Synchronization or XA enlistment. Note that recovery isn't supported yet.&lt;/li&gt;&lt;li&gt;Caches can now configure the maximum number of attempts to start a CacheWriter/CacheLoader on startup before cache creation fails.&lt;/li&gt;&lt;li&gt;Write-behind stores are now fault-tolerant by default.&lt;/li&gt;&lt;li&gt;Segmented On Heap Data Container. It improves performance of stream operations.&lt;/li&gt;&lt;li&gt;Server upgraded to Wildfly 13.&lt;/li&gt;&lt;li&gt;We have introduced several WildFly feature packs to make it easier for Infinispan to be utilised on WildFly instances via the Server Provisioning Plugin. The following feature packs have been created, most notably:&lt;/li&gt;&lt;ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-client&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;All of the modules required to connect to a hotrod server via the client&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-embedded&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;The modules required for embedded instances of Infinispan&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-embedded-query&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;The same as above but with query capabilities&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;infinispan-feature-pack-wf-modules&lt;/span&gt;&lt;/li&gt;&lt;ul&gt;&lt;li&gt;This is equivalent to the Wildfly-modules.zip&lt;/li&gt;&lt;/ul&gt;&lt;/ul&gt;&lt;li&gt;Hibernate second-level cache provider works with Hibernate ORM 5.3.&lt;/li&gt;&lt;li&gt;The Hot Rod Server allows now to use multiple protocols with a Single Port. The initial version supports HTTP/1.1, HTTP/2 and Hot Rod. Switching protocols can be done using TLS/ALPN and HTTP/1.1 Upgrade header.&lt;/li&gt;&lt;li&gt;Admin console - improved all editors (schema, scripts, JSON data) to include syntax highlighting.&lt;/li&gt;&lt;li&gt;Several enhancements in the Java Hot Rod client allowing to read and write data in different formats such as JSON, for cache operations and deployed filters/converters.&lt;/li&gt;&lt;li&gt;Cluster wide max idle expiration.&lt;/li&gt;&lt;li&gt;Component Upgrades&lt;/li&gt;&lt;ul&gt;&lt;li&gt;Hibernate Search 5.10&lt;/li&gt;&lt;li&gt;Hibernate ORM 5.3&lt;/li&gt;&lt;/ul&gt;&lt;li&gt;Numerous bug fixes which improve stability&lt;/li&gt;&lt;/ul&gt;&lt;div&gt;For more details, please check our &lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12310799&amp;amp;version=12336209"&gt;issue tracking release notes&lt;/a&gt;.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;Thanks to everyone involved in this release! Onward to Infinispan 9.4!&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Cheers,&lt;/div&gt;&lt;div&gt;Galder&lt;/div&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/Infinispan/~4/7_fbY5uj-Uk" height="1" width="1" alt=""/&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rvx4rV1zi7U" height="1" width="1" alt=""/&gt;</content><summary>We're delighted to announce the release of Infinispan 9.3.0.Final, which is a culmination of several months of hard work by the entire Infinispan community. Here's a summary of what you can find within it: First final release to work with both Java 8 and Java 10. Note that Infinispan only works in classpath mode. Transaction support Hot Rod. The java Hot Rod client can participate in Java transact...</summary><dc:creator>Galder Zamarreño</dc:creator><dc:date>2018-06-26T07:12:00Z</dc:date><feedburner:origLink>http://feedproxy.google.com/~r/Infinispan/~3/7_fbY5uj-Uk/infinispan-930final-is-out.html</feedburner:origLink></entry><entry><title>Hibernate Community Newsletter 12/2018</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/o3kxMQACy_w/" /><category term="Discussions" scheme="searchisko:content:tags" /><category term="feed_group_name_hibernate" scheme="searchisko:content:tags" /><category term="feed_name_inrelationto" scheme="searchisko:content:tags" /><category term="Hibernate ORM" scheme="searchisko:content:tags" /><category term="newsletter" scheme="searchisko:content:tags" /><author><name>Vlad Mihalcea</name></author><id>searchisko:content:id:jbossorg_blog-hibernate_community_newsletter_12_2018</id><updated>2018-06-25T08:18:10Z</updated><published>2018-06-25T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Welcome to the Hibernate community newsletter in which we share blog posts, forum, and StackOverflow questions that are especially relevant to our users.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="articles"&gt;&lt;a class="anchor" href="#articles"&gt;&lt;/a&gt;Articles&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In &lt;a href="https://rieckpil.de/how-to-lazy-loading-of-jpa-attributes-with-hibernate/"&gt;this article&lt;/a&gt;, Philip Riecks explains how to improve performance by lazy loading entity attributes.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;When using JPA and Hibernate, it can happen to overlook some basic concepts which are actually very important when dealing with performance issues. &lt;a href="https://vladmihalcea.com/entitymanager-find-getreference-jpa/"&gt;This article&lt;/a&gt; explains the difference between the &lt;code&gt;find&lt;/code&gt; and &lt;code&gt;getReference&lt;/code&gt; method of the &lt;code&gt;EntityManager&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you’re planning to use Spring Data, the following two tutorials will help you get started:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.javacodegeeks.com/2018/05/spring-data-jpa-tutorial.html"&gt;Spring Data JPA Tutorial by Anand Kumar&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://medium.com/skillhive/spring-boot-spring-data-jpa-and-oracle-database-c4af89f727e0"&gt;Developing RESTful web service using Spring Boot, Spring Data JPA and Oracle Database by Ikhiloya Imokhai&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;One advantage of using JPA and Hibernate is the support for database and application-level concurrency control. In these two articles, Eugen Paraschiv explains how to use:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.baeldung.com/jpa-optimistic-locking"&gt;Optimistic locking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://www.baeldung.com/jpa-pessimistic-locking"&gt;Pessimistic locking&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;On the same topic, it’s also worth reading &lt;a href="https://vladmihalcea.com/non-repeatable-read/"&gt;this article&lt;/a&gt; that explains the Non-Repeatable Read data anomaly.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="time-to-upgrade"&gt;&lt;a class="anchor" href="#time-to-upgrade"&gt;&lt;/a&gt;Time to upgrade&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;There Hibernate Search &lt;a href="http://in.relation.to/2018/06/22/hibernate-search-5-10-2-Final/"&gt;5.10.2&lt;/a&gt; has been released.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="questions-and-answers"&gt;&lt;a class="anchor" href="#questions-and-answers"&gt;&lt;/a&gt;Questions and answers&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/twowayfieldbridge-on-composite-id-referring-to-another-entity/889"&gt;TwoWayFieldBridge on composite ID referring to another entity when using Hibernate Search&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/is-it-true-that-jpas-criteriaquery-uses-jpql/955"&gt;Is it true that JPA’s CriteriaQuery uses JPQL?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/unidirectional-vs-bidirectional-manytoone/951"&gt;Unidirectional vs bidirectional ManyToOne&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/performance-benchmark-for-hibernate/922"&gt;Performance benchmark for Hibernate&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/cachekey-class-missing-from-hibernate-5-jar/949/2"&gt;CacheKey class missing from Hibernate 5 API&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/hibernate-5-programatically-adding-hbm-mapping-to-the-configuration-no-named-queries-found/948/7"&gt;Hibernate 5 - programmatically adding HBM mapping to the configuration - no named queries found&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/session-createfilter-is-deprecated-in-hibernate-orm-5-3-is-there-any-alternative/940"&gt;Session.createFilter is deprecated in Hibernate ORM 5.3. Is there any alternative?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/hql-deletion-does-not-trigger-elasticsearch-deletion/944/2"&gt;HQL deletion does not trigger Elasticsearch deletion&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/hibernate-with-mysql-throws-wrongargumentexception-malformed-database-url-failed-to-parse-the-connection-string-near-usejdbccomplianttimezoneshift-uselegacydatetimecode-servertimezone/931"&gt;Hibernate with MySQL throws “WrongArgumentException: Malformed database URL, failed to parse the connection string near: useJDBCCompliantTimezoneShift, useLegacyDatetimeCode, serverTimezone”&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/t/how-to-make-proxies-implement-my-marker-interface/921"&gt;How to make Hibernate Proxies implement a certain marker interface&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stackoverflow.com/questions/1607532/when-to-use-entitymanager-find-vs-entitymanager-getreference/50945279#50945279"&gt;When to use EntityManager.find() vs EntityManager.getReference() with JPA&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stackoverflow.com/questions/50903883/does-using-constructor-result-in-a-hibernate-query-affect-performance/50904382#50904382"&gt;Does using Constructor Result in a JPA or Hibernate query affect performance?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stackoverflow.com/questions/50894094/hibernate-automatically-changed-generationtype-based-database-type/50908319#50908319"&gt;How to change Hibernate GenerationType identifier depending on the underlying database&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stackoverflow.com/questions/51008884/hibernate-returns-null-for-entity-property-that-is-annotated-with-formula-after/51011150#51011150"&gt;Hibernate returns null for entity property that is annotated with @Formula after saving the entity&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stackoverflow.com/questions/28486850/what-is-the-difference-between-a-session-and-a-connection-in-hibernate/28495180#28495180"&gt;What is the difference between a Session and a Connection in Hibernate?&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/o3kxMQACy_w" height="1" width="1" alt=""/&gt;</content><summary>Welcome to the Hibernate community newsletter in which we share blog posts, forum, and StackOverflow questions that are especially relevant to our users. Articles In this article, Philip Riecks explains how to improve performance by lazy loading entity attributes. When using JPA and Hibernate, it can happen to overlook some basic concepts which are actually very important when dealing with perform...</summary><dc:creator>Vlad Mihalcea</dc:creator><dc:date>2018-06-25T00:00:00Z</dc:date><feedburner:origLink>http://in.relation.to/2018/06/25/hibernate-community-newsletter-2018-12/</feedburner:origLink></entry><entry><title>Version 1.4 of Apiman is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/aPwnEgq5TFc/release-1.4.html" /><category term="feed_group_name_apiman" scheme="searchisko:content:tags" /><category term="feed_name_apiman" scheme="searchisko:content:tags" /><author><name>Marc Savy</name></author><id>searchisko:content:id:jbossorg_blog-version_1_4_of_apiman_is_released</id><updated>2018-06-22T18:40:00Z</updated><published>2018-06-22T18:40:00Z</published><content type="html">&lt;div class="paragraph"&gt; &lt;p&gt;I&amp;#8217;m delighted to announce that Apiman 1.4 has been released (actually, 1.4.1.Final as of this blog post &lt;sup class="footnote"&gt;[&lt;a id="_footnoteref_1" class="footnote" href="#_footnotedef_1" title="View footnote."&gt;1&lt;/a&gt;]&lt;/sup&gt;).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The most important change in this release is that we&amp;#8217;ve upgraded support for Elasticsearch from 1.x to 5.x. It may also support Elasticsearch 2.x, but this isn&amp;#8217;t officially supported (let us know your experiences).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A significant number of changes across the ES platform were needed to bring this improvement; including in Apiman Gateway, Apiman Manager, Apiman Metrics, test harnesses, and the ES distribution.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you experience any issues, please report them to us via &lt;a href="https://issues.jboss.org/browse/APIMAN/"&gt;JIRA&lt;/a&gt;, &lt;a href="https://github.com/apiman/apiman"&gt;GitHub&lt;/a&gt;, or &lt;a href="https://lists.jboss.org/mailman/listinfo/apiman-user"&gt;the mailing list&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="upgrading"&gt;Upgrading&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Depending on your approach, to upgrade you can simply use Apiman&amp;#8217;s &lt;a href="http://www.apiman.io/blog/apiman/introduction/overview/backup/export/import/2016/01/27/export-import.html"&gt;export-import feature&lt;/a&gt;, or upgrade the indices by following Elasticsearch&amp;#8217;s upgrade guides (likely trickier; I recommend export-import).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We also now launch and manage ES in a significantly different way than previously (as an external process), as the &lt;a href="https://www.elastic.co/blog/elasticsearch-the-server"&gt;old method is no longer supported&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="policies-can-suppressallow-headers-in-connectors"&gt;Policies can suppress/allow headers in connectors.&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;A new function has been added to &lt;code&gt;IPolicyContext&lt;/code&gt; which enables policy authors to explicitly suppress or allow headers that may otherwise have different treatment by default.&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="java"&gt;IConnectorConfig getConnectorConfiguration();&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Using &lt;code&gt;IConnectorConfig&lt;/code&gt; a policy author may explicitly override the connector&amp;#8217;s default filtering of headers. These may vary slightly by platform, but generally would by default filter out headers such as &lt;code&gt;X-Api-Key&lt;/code&gt;. This is applied at the &lt;strong&gt;end of the policy chain&lt;/strong&gt; right before the connection is established.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This feature is useful to unblock certain headers that may otherwise be disallowed, or block headers in such a way that it would even apply to subsequent policies.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Here&amp;#8217;s an example, using the &lt;code&gt;suppressRequestHeader&lt;/code&gt; method:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="java"&gt;&lt;span class="type"&gt;void&lt;/span&gt; doApply(ApiRequest request, IPolicyContext context, ...) { &lt;span class="comment"&gt;// Get connector config&lt;/span&gt; IConnectorConfig connectorConfig = context.getConnectorConfiguration(); &lt;span class="comment"&gt;// Ban header. Connector will filter this out.&lt;/span&gt; connectorConfig.suppressRequestHeader(&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;X-SECRET&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;); &lt;span class="local-variable"&gt;super&lt;/span&gt;.doApply(request, context, config, chain); }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Even if another policy in the chain added an &lt;code&gt;X-SECRET&lt;/code&gt; header, it would still be filtered out &lt;sup class="footnote"&gt;[&lt;a id="_footnoteref_2" class="footnote" href="#_footnotedef_2" title="View footnote."&gt;2&lt;/a&gt;]&lt;/sup&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;We hope to expand the functionality of &lt;code&gt;IConnectorConfig&lt;/code&gt; in future to allow more control of the connector by policies than is possible presently.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="download-1-4-1-final"&gt;Download 1.4.1.Final&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-vertx-1.4.1.Final.zip"&gt;Vert.x (Gateway Only)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-wildfly10-1.4.1.Final-overlay.zip"&gt;WildFly 10 or EAP 7.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-wildfly11-1.4.1.Final-overlay.zip"&gt;WildFly 11&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-eap7-1.4.1.Final-overlay.zip"&gt;EAP 7&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://downloads.jboss.org/apiman/1.4.1.Final/apiman-distro-tomcat8-1.4.1.Final-overlay.zip"&gt;Tomcat 8+&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="release-notes"&gt;Release Notes&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12314121&amp;amp;version=12337953"&gt;1.4.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://issues.jboss.org/secure/ReleaseNote.jspa?projectId=12314121&amp;amp;version=12338072"&gt;1.4.1.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Enhancements&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1334"&gt;APIMAN-1334&lt;/a&gt; - Allow policies to suppress/allow headers in connector.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Bugs&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1266"&gt;APIMAN-1266&lt;/a&gt; - Error when adding plugin that has previously been deleted (Oracle 12C database)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1296"&gt;APIMAN-1296&lt;/a&gt; - The API Key policy plugin (apikey-policy) expects the requestHeader property to be all lowercase as of 1.3.1&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1318"&gt;APIMAN-1318&lt;/a&gt; - Export/Import of a plugin with policies does not work because of its ID&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1320"&gt;APIMAN-1320&lt;/a&gt; - Gateway API: Clients still inserted even when invalid&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1321"&gt;APIMAN-1321&lt;/a&gt; - Elasticsearch data is deleted at tomcat shutdown&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1324"&gt;APIMAN-1324&lt;/a&gt; - index_already_exists_exception when starting WF quickstart&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1335"&gt;APIMAN-1335&lt;/a&gt; - ApiKeyPolicy from apikey-policy is throwing an NPE on null connectorConfig using the Vert.x gateway&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1337"&gt;APIMAN-1337&lt;/a&gt; - SoapAuthorizationPolicy missing i18n messages&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Tasks&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-892"&gt;APIMAN-892&lt;/a&gt; - Upgrade to Elasticsearch 5.x&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;div class="title"&gt;Sub-tasks&lt;/div&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1323"&gt;APIMAN-1323&lt;/a&gt; - Rework test harness to cope better with out of order JSON&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1325"&gt;APIMAN-1325&lt;/a&gt; - Upgrade Jest HTTP Client for ES&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1326"&gt;APIMAN-1326&lt;/a&gt; - Upgrade Gateway to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1327"&gt;APIMAN-1327&lt;/a&gt; - Upgrade Manager to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1328"&gt;APIMAN-1328&lt;/a&gt; - Upgrade metrics to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1329"&gt;APIMAN-1329&lt;/a&gt; - Update test harness and tests to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1330"&gt;APIMAN-1330&lt;/a&gt; - Update QueryBuilders to ES 5.x&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1331"&gt;APIMAN-1331&lt;/a&gt; - Switch to EmbeddedElastic instead of (actually) embedded&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1332"&gt;APIMAN-1332&lt;/a&gt; - Update ES distro to include Elasticsearch distro zip&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://issues.jboss.org/browse/APIMAN-1333"&gt;APIMAN-1333&lt;/a&gt; - Handle EmbeddedElastic hanging process when JVM killed ungracefully.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id="footnotes"&gt; &lt;hr&gt; &lt;div class="footnote" id="_footnotedef_1"&gt; &lt;a href="#_footnoteref_1"&gt;1&lt;/a&gt;. We fixed a couple of bugs spotted in 1.4.0.Final by the community before the blog was written &lt;/div&gt; &lt;div class="footnote" id="_footnotedef_2"&gt; &lt;a href="#_footnoteref_2"&gt;2&lt;/a&gt;. Unless it explicitly overrode your suppression! &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/aPwnEgq5TFc" height="1" width="1" alt=""/&gt;</content><summary>I’m delighted to announce that Apiman 1.4 has been released (actually, 1.4.1.Final as of this blog post [1]). The most important change in this release is that we’ve upgraded support for Elasticsearch from 1.x to 5.x. It may also support Elasticsearch 2.x, but this isn’t officially supported (let us know your experiences). A significant number of changes across the ES platform were needed to bring...</summary><dc:creator>Marc Savy</dc:creator><dc:date>2018-06-22T18:40:00Z</dc:date><feedburner:origLink>http://apiman.io/blog/apiman/release/2018/06/22/release-1.4.html</feedburner:origLink></entry><entry><title>DesOps – The Next Wave in Design</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Gg4Eb2ggcUk/" /><category term="DevOps" /><category term="UI/UX" /><category term="Agile" /><category term="design" /><category term="DesignOps" /><category term="DesOPs" /><author><name>Samir Dash</name></author><id>https://developers.redhat.com/blog/?p=503277</id><updated>2018-06-22T12:46:04Z</updated><published>2018-06-22T12:46:04Z</published><content type="html">&lt;p&gt;&lt;em&gt;DesOps,&lt;/em&gt; aka. &lt;em&gt;DesignOps, &lt;/em&gt;refers to an approach to design that is inspired by the culture of &lt;em&gt;DevOps&lt;/em&gt;. In this and the following posts, we will explore, the practical approaches for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How to prepare for the next wave in design that compliments the &lt;em&gt;DevOps&lt;/em&gt; concepts of a cultural shift, collaboration, and automation.&lt;/li&gt; &lt;li&gt;We will also see what solutions are available today that contribute to bringing the full circle of design in the context of the software development lifecycle.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Today, &lt;em&gt;design&lt;/em&gt; as a discipline is getting more and more recognition across the entrepreneur world and many industry efforts, such as &lt;a href="https://www.ibm.com/design/"&gt;&lt;em&gt;IBM&lt;/em&gt;&amp;#8216;s &lt;em&gt;Enterprise Design Thinking&lt;/em&gt;&lt;/a&gt; framework, &lt;a href="https://openstudio.redhat.com/designing-the-open-source-way/"&gt;&lt;em&gt;RedHat&lt;/em&gt;&amp;#8216;s &lt;em&gt;Open Studio&lt;/em&gt;&lt;/a&gt; and similar ones, are at a large scale, trying to create a synergy between the &lt;em&gt;Agile&lt;/em&gt; approach to the software development lifecycle and the&lt;em&gt; Design Thinking&lt;/em&gt;.  It is an interesting crossroad where the next big thing in product delivery is to bring scalability as well as automation to the creative process.&lt;/p&gt; &lt;p&gt;In the context of the software industry, I always see “design” as an intersection between creativity and technology where both shape each other—with help from user needs—and blend the results into successful products.&lt;/p&gt; &lt;p&gt;Any typical software product that is delivered involves many complex as well as divergent technologies, processes, people, and visions. Though software delivery mostly happens with team members segmented into two major groups—developers and designers—ultimately, the best outcome always depends on how the two teams communicate with each other and how efficiently their thoughts and ideas are shared, propagated, and translated.&lt;/p&gt; &lt;p&gt;&lt;span id="more-503277"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;When it comes to product development, the amount of complexity and the variety of aspects—from diversified thinking, technology, tools, and processes—that go into it, are significant. Attempts have been made over to improve various aspects to ensure the delivery process can be optimized to scale up to ever-expanding needs. In the software and IT infrastructure industry, recently one such phenomenon is DevOps, which focused on rethinking development and operations to improve productivity and efficiency.&lt;/p&gt; &lt;p&gt;DevOps started near the end of the first decade of this century. Back in 2008, there was a fine separation between the roles of who would code and who would deploy software. Basically, the coders were responsible for code generation while the infrastructure guys looked after the process of deploying software.&lt;/p&gt; &lt;p&gt;Due to the rise of the Agile process, code generation and deployment as a part of delivery became more frequent and continuous, unlike the age-old waterfall model when the cycle used to happen every six months to a year. In all major software services industry, it was common to have fixed calendar dates that represented software releases. The two-to-three-week sprints of the &lt;em&gt;Agile&lt;/em&gt; approach made such cycles obsolete in many ways.&lt;/p&gt; &lt;p&gt;As continuous delivery became the de facto standard, it narrowed the gap between the development team and the infrastructure team. This change also gave rise to the need for multidisciplinary roles or individuals who could bridge the gap between the production environment and the development server, allowing their code to be deployed in a more efficient way and faster. As the &lt;em&gt;DevOps&lt;/em&gt; movement took shape, the practices around it grew from a few talented hackers to a profession with a culture of its own involving its own set of tools, practices, technologies, and workflows that has become the norm in the industry today.&lt;/p&gt; &lt;p&gt;Today, most of the &lt;em&gt;DevOps&lt;/em&gt; approach focuses on the process blocks that mostly impact engineering or technical aspects of the product rather than on the design aspect. To bridge that gap, many attempts are being made to define a consistent approach called DesOps&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;DesOps&lt;/em&gt; or &lt;em&gt;DesignOps&lt;/em&gt; is a relatively new term. To comprehend &lt;em&gt;DesOps&lt;/em&gt; better, many refer to &lt;em&gt;DevOps&lt;/em&gt;, which has similar underlying philosophies and goals. DesOps is a relatively new concept, yet it is a growing area of concern for designers seeking to help their teams increase the value they produce for their organizations and their organization’s customers. However, DesOps practices have been inconsistent in many attempts made by different organizations for many years.&lt;/p&gt; &lt;p&gt;Even when we try to implement a &lt;em&gt;DevOps&lt;/em&gt;-geared process to run a design-driven process model, the challenges, such as the gaps between design and development or between design and testing, are not fixed. Without implementing DesOps to fix the design process, the implementation of DevOps&lt;em&gt; &lt;/em&gt;will never yield the desired outcome and will not be able to sustain the core philosophies behind it.&lt;/p&gt; &lt;div class="slate-resizable-image-embed slate-image-embed__resize-full-width"&gt;&lt;img class=" size-full wp-image-503297 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops.png" alt="" width="360" height="360" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops.png 360w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-150x150.png 150w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-300x300.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-32x32.png 32w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-50x50.png 50w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-64x64.png 64w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-96x96.png 96w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/desops-128x128.png 128w" sizes="(max-width: 360px) 100vw, 360px" /&gt;&lt;/div&gt; &lt;p&gt;&lt;em&gt;DesOps&lt;/em&gt; and &lt;em&gt;DevOps&lt;/em&gt; are complementary to each other. The design delivery process improvements try to optimize the overall delivery process and, thereby, contribute to &lt;em&gt;DevOps&lt;/em&gt;. For example, aspects such as the testing of the product involve design aspects, usability, accessibility, and so on. In the testing phase, benchmarks are needed to act as a referee; that can only come from a process where &lt;em&gt;DesOps&lt;/em&gt; has implemented the output and then feeds the benchmark to the &lt;em&gt;DevOps&lt;/em&gt; phase where the testing block can use it. In addition, when we use the Agile or iterative delivery process models, at each sprint cycle the end-to-end flow is executed, thereby making &lt;em&gt;Continous Integration&lt;/em&gt; (CI) and &lt;em&gt;Continous Delivery&lt;/em&gt; (CD) truly meaningful.&lt;/p&gt; &lt;p&gt;&lt;em&gt;DesOps&lt;/em&gt; was primarily born out of the primary need of how to design at scale. The factors that shaped it are of similar to what shaped &lt;em&gt;DevOps&lt;/em&gt;. With software delivery in recent times, with the &lt;em&gt;Agile&lt;/em&gt; process, &lt;em&gt;CI&lt;/em&gt; &amp;#38; &lt;em&gt;CD&lt;/em&gt; including  &lt;em&gt;Continuous Deployment&lt;/em&gt;, the &lt;em&gt;DevOps&lt;/em&gt; approach provides a faster highway that ensures faster delivery with lower risks. So the earlier software development lifecycle model got redefined from Agile to &lt;em&gt;DevOps&lt;/em&gt; in its current shape.&lt;/p&gt; &lt;p&gt;However, because the design is an integral part of any delivered product, the gaps between the traditional design lifecycle and the fast-track &lt;em&gt;DevOps&lt;/em&gt; development lifecycle need to be bridged. The need for tighter integration between the design team and the engineering team is a necessity to ensure design at scale. In the past two or three years, the top five big technology companies have made heavy investments in this area and have paved the way for other organizations and design communities to be more explorative in this area. The implications of  &lt;em&gt;DesOps&lt;/em&gt; are reflected in the outcome, where the silos among the teams and disciplines are reduced. Along with this, &lt;em&gt;DesOps&lt;/em&gt; improves the collaboration among cross-functional teams and working practices, which contributes to minimizing waste in the delivery process.&lt;/p&gt; &lt;p&gt;Historically and interestingly, in the beginning, &lt;em&gt;DesOps&lt;/em&gt;—without its formal name—focused on the areas of creation and maintenance and the sharing of its modular design systems. In the last couple of years, it has been more about organizations having design systems and making these socialized. Primarily, these design systems have consisted of visual design languages and components and widgets. These design systems have defined basic goals, principles, branding (for specific organizational identity), and a visual language that helped maintain consistency in the creation of design artefacts and assets. Along with that, the UI patterns and widget libraries included helped to bring consistency in terms of interactions across a wider scale of interfaces within an organization or product portfolio.&lt;/p&gt; &lt;p&gt;Ensuring this consistency became part of a strategic aspect of an organization&amp;#8217;s user experience goals, and the design team was responsible for driving this. Mostly this task became a primary part of the roles of &lt;em&gt;Design Director&lt;/em&gt;, &lt;em&gt;Lead,&lt;/em&gt; and &lt;em&gt;Principal&lt;/em&gt; roles in an organization as a part of their goal to ensure bringing the right maturity to their design team and practices.&lt;/p&gt; &lt;p&gt;This was definitely low-hanging fruit in terms of what the &lt;em&gt;DesOps&lt;/em&gt; principle is geared towards. The return from such low-hanging fruit was helpful in many ways. Apart from consistency, it helped reduce the friction among teams regarding the design aspect. Also, it helped to reduce some aspects of operational inefficiencies in the design workflow and to reduce waste, thereby helping the team to deliver at a faster rate.&lt;/p&gt; &lt;p&gt;However, design work practices, unlike the practices of the development domain, are more diverse and design is the area with the most creative energy in the whole lifecycle. So, the challenges of ensuring the smooth amalgamation of the design systems into the process blocks of the workflow were not easy. The fact is, it is still a challenge to fit the existing tools to the design workflow and then align that to the whole delivery track fueled by the &lt;em&gt;DevOps&lt;/em&gt; paradigm.&lt;a href="https://airbnb.design/painting-with-code/"&gt;&lt;code&gt;&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently, the design team at Airbnb came up with the &lt;a href="https://airbnb.design/painting-with-code/"&gt;&lt;em&gt;React-Sketch App&lt;/em&gt;&lt;/a&gt; open source library. Last year,  at Red Hat UX team meet-up summit, as a part of a design challenge initiative, I presented a concept called &lt;em&gt;&lt;a href="http://desops.io/2018/05/12/video-ditto-design-life-cycle-management-concept-for-desops-2016-17/"&gt;Ditto&lt;/a&gt;,&lt;/em&gt; which was supposed to redefine the way the design can be integrated into a &lt;em&gt;DevOps &lt;/em&gt;supported environment. I will share the details of &lt;em&gt;Ditto&lt;/em&gt; in future articles in this series. Recently, &lt;a href="https://clearleft.com/posts/fractal-v1-0"&gt;Clearleft&lt;/a&gt; came up with the &lt;em&gt;Fractal&lt;/em&gt; open source tool, which tried to reduce and even remove the distance between the design and development teams. Note that both &lt;em&gt;DevOps&lt;/em&gt; and &lt;em&gt;DesOps &lt;/em&gt;were born out of similar drivers; however, the practices concerned with the two are very different.&lt;/p&gt; &lt;p&gt;From the example of Salesforce’s approach to design, the takeaway is that the technological approach of using “the single source of truth” can be a good starting point towards building a practical &lt;em&gt;DesOps&lt;/em&gt; culture in an organization. Because the soul of &lt;em&gt;DesOps &lt;/em&gt;is based on the cultural shift and practices that work towards &lt;em&gt;CI&lt;/em&gt; and &lt;em&gt;CD&lt;/em&gt;, it makes sense to use living design systems as the foundation of the overarching concept of &lt;em&gt;DesOps&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;To understand the overarching structure of &lt;em&gt;DesOps&lt;/em&gt;, we need to explore the various dimensions that give the concept its shape. From a framework point of view, the typical three pillars of any framework also fit here:&lt;/p&gt; &lt;p&gt;&lt;img class=" size-full wp-image-503307 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/06/0-1-1024x709.jpeg" src="https://developers.redhat.com/blog/wp-content/uploads/2018/06/0-1.jpeg" alt="" width="1920" height="1329" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/06/0-1.jpeg 1920w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/0-1-300x208.jpeg 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/0-1-768x532.jpeg 768w, https://developers.redhat.com/blog/wp-content/uploads/2018/06/0-1-1024x709.jpeg 1024w" sizes="(max-width: 1920px) 100vw, 1920px" /&gt;&lt;/p&gt; &lt;p&gt;1. &lt;strong&gt;Consistency:&lt;/strong&gt; In the context of &lt;em&gt;DesOps&lt;/em&gt;,&lt;em&gt; &lt;/em&gt; consistency plays the major role both in the approach and the workflows as well as from the design perspective.&lt;/p&gt; &lt;p&gt;2.&lt;strong&gt; Continuity.&lt;/strong&gt; This mostly fuels the continuous design aspect that provides agility to the design process.&lt;/p&gt; &lt;p&gt;3. &lt;strong&gt;Complimentary.&lt;/strong&gt; There is no doubt that as &lt;em&gt;DesOps &lt;/em&gt;completes the full circle, it complements the vision of &lt;em&gt;DevOps. &lt;/em&gt;&lt;/p&gt; &lt;p&gt;In my view, &lt;em&gt;DesOps&lt;/em&gt; is not only about the tools or technology employed—which generally refers to the discourses for bringing automation and improved process re-engineering in the engineering sense; rather, &lt;em&gt;DesOps&lt;/em&gt; is about defining a culture of design through improved work practices;  communication between the design teams and within and outside the project/product teams along with the stakeholders; and about aiding these practices—with the help of technology to enhance communication among the involved tools and the overarching eco-systems.&lt;/p&gt; &lt;p&gt;Stay tuned and be part of the &lt;em&gt;DesOps&lt;/em&gt; journey with me.&lt;/p&gt; &lt;h6&gt;&lt;/h6&gt; &lt;h6&gt;&lt;strong&gt;(Note: Based on my book &lt;em&gt;The DesOps Enterprise: Overview &amp;#38; Culture&lt;/em&gt;)&lt;/strong&gt;&lt;/h6&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;linkname=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F06%2F22%2Fdesops-the-next-wave-in-design%2F&amp;#38;title=DesOps%20%E2%80%93%20The%20Next%20Wave%20in%20Design" data-a2a-url="https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/" data-a2a-title="DesOps – The Next Wave in Design"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/"&gt;DesOps &amp;#8211; The Next Wave in Design&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Gg4Eb2ggcUk" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;DesOps, aka. DesignOps, refers to an approach to design that is inspired by the culture of DevOps. In this and the following posts, we will explore, the practical approaches for How to prepare for the next wave in design that compliments the DevOps concepts of a cultural shift, collaboration, and automation. We will also see what solutions are available today [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/"&gt;DesOps &amp;#8211; The Next Wave in Design&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">503277</post-id><dc:creator>Samir Dash</dc:creator><dc:date>2018-06-22T12:46:04Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/06/22/desops-the-next-wave-in-design/</feedburner:origLink></entry></feed>
